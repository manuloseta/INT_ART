{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AI Lab Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Building a classifier on a real dataset (3.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pima.csv\", header=0, sep=',')\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal is to predict whether or not a patient has diabetes from the values ​​of some variables. The target variable is \"class\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Pregnancies:** Number of times pregnant\n",
    "* **Glucose:** Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "* **BloodPressure:** Diastolic blood pressure (mm Hg)\n",
    "* **SkinThickness:** Triceps skin fold thickness (mm)\n",
    "* **Insulin:** 2-Hour serum insulin (mu U/ml)\n",
    "* **BMI:** Body mass index (weight in kg/(height in m)^2)\n",
    "* **DiabetesPedigreeFunction:** Diabetes pedigree function\n",
    "* **Age:** Age (years)\n",
    "* **Class:** Class variable (\"yes\" / \"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(df.columns)\n",
    "feature_names.remove('class')\n",
    "print(feature_names)\n",
    "X = df[feature_names].values\n",
    "y = df['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic stats for each attribute:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Smoothed histograms of each attribute in each class. Color indicates class (\"yes\"/\"no\"):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for i,n in enumerate(feature_names):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    aux = 'Density' if i%4==0 else ''\n",
    "    df.groupby(\"class\")[n].plot(kind='kde', title='Hist. de '+n)\n",
    "    plt.ylabel(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model and testing its quality using 5-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains a model and tests it on several different training-test partitions of the data. The result is a mean score with its standard deviation. The type of model (Naïve Bayes / decision tree / knn / logistic regression / neural network) and parameters used must be selected to obtain the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# other classifiers (from notebook p4_01)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1) # DecisionTreeClassifier(max_depth=3)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(\"All scores: \", scores)\n",
    "print(\"Global Model Score: {:.2f} +/- {:.2f}\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Answer the following questions here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the best score you get with a k-nn and with what k (value of n_neighbours)?\n",
    "* What is the best score you get with a decision tree and at what maximum depth (value of max_depth)?\n",
    "* What is the best score you get with a neural network and with what configuration (value of hidden_layer_sizes)?\n",
    "\n",
    "Note: to answer these questions you just have to change the type of model and its parameters in the previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model: feature processing and parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, instead of using more complex models, it is more useful to spend more time processing the data to get better results.\n",
    "\n",
    "In this section you will investigate a few approaches for preparing the data which are likely to improve the results: feature construction and selection, feature preprocessing (detection of outliers, missing values, centering and scaling).\n",
    "\n",
    "Give reasons why you decide to try or ignore any of these methods, and how the results change when you apply them (you can create as many cells as you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think that another configuration of the classifier hyperparameters can solve the problem more efficiently? \n",
    "Most likely. \n",
    "\n",
    "Now try to change the value of the hyperparameters and return as the final classifier the one that minimizes the estimation of the generalization error. To do this, you have to do two things. The first one is to change the way in which we estimate the generalization error. If we base our results on the error provided by the test, we will overfit the test set. Therefore we must change this estimate. We will estimate the generalization error of each classifier using Nested Cross Validation. \n",
    "On the other hand, we will do a grid search of the optimal hyperparameters. We will return the value of the hyperparameters that optimize that error estimate. \n",
    "\n",
    "Adapt the code found at https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html to this problem and to the hyperparameter space of one of the classifiers. \n",
    "Remember that at https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier and at https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier you have information about each of the hyperparameters. You are free to choose the values ​​and hyperparameters to consider. Before configuring the grid, read about each one of the hyperparameters to make sure your search makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include code about this section here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
